<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Engagement Drop Root Cause Analysis | Case Study</title>

  <!-- Base design system -->
  <link rel="stylesheet" href="styles/main.css">
</head>
<body>

  <!-- NAVBAR -->
  <nav>
    <div class="left">Abhitay Shinde</div>
    <div class="right">
      <a href="index.html#projects">Projects</a>
      <a href="index.html#experience">Background</a>
      <a href="index.html#contact">Contact</a>
      <a class="btn" href="../resume.pdf" target="_blank">Download Resume</a>
    </div>
  </nav>

  <!-- HERO / INTRODUCTION -->
  <section class="hero">
    <h1>Engagement Drop Root Cause Analysis: An Exec-Facing AI Decision System</h1>
    <div class="tech-tags">
      <span class="tag">Python</span>
      <span class="tag">Pandas</span>
      <span class="tag">Causal Inference</span>
      <span class="tag">DiD</span>
      <span class="tag">Propensity Score Matching</span>
      <span class="tag">Statistical Testing</span>
      <span class="tag">Gemini API</span>
      <span class="tag">Product Analytics</span>
    </div>
    <p>
      I built an end-to-end product data science system to diagnose an executive-level engagement drop, moving from raw signals and misleading correlations to a defensible, decision-ready recommendation supported by causal reasoning and bias correction.
    </p>
    <!-- TODO: Insert high-level system flow diagram (Metrics ‚Üí Analysis ‚Üí Decision ‚Üí AI Copilot) -->
  </section>

  <!-- TL;DR Decision Box -->
  <section>
    <div class="recommendation danger">
      <h3>TL;DR ‚Äî Product Decision</h3>
      <ul>
        <li>üìâ Engagement (ESAU) dropped ~8% over 4 weeks</li>
        <li>‚ö†Ô∏è Naive analysis blamed a newly launched feature</li>
        <li>üìà Statistical tests and DiD appeared significant ‚Äî but were misleading</li>
        <li>üî¨ After correcting for selection bias and confounders:</li>
        <ul>
          <li>~3% driven by seasonality</li>
          <li>~4% driven by cohort quality shift</li>
          <li>~1% attributable to a feature interaction bug</li>
        </ul>
      </ul>
      <p><strong>Decision:</strong> Do not rollback the feature; apply a targeted fix and monitor.</p>
      <!-- TODO: Embed ‚ÄúDaily Sessions / ESAU (Exec View)‚Äù plot here. Export from data_creation.ipynb as PNG to docs/assets/ and embed below. -->
      <!-- <img src="assets/daily_sessions_esau.png" alt="Daily Sessions / ESAU (Exec View)">
      <div class="caption">ESAU trend over 90 days (exec view). Source: data_creation.ipynb</div> -->
    </div>
  </section>

  <!-- PRODUCT INFORMATION -->
  <section>
    <h2>Product Information</h2>
    <h3>Product Context</h3>
    <p>
      The simulated product is a B2C, feed-based application where users engage through sessions and content discovery. Leadership monitors engagement closely due to its downstream impact on retention and revenue. A new feed feature (SmartFeed_v2) was rolled out gradually, shortly before engagement began to decline, creating immediate concern that the feature caused a regression.
    </p>
    <h3>Tech Stack</h3>
    <ul>
      <li>Python ¬∑ Pandas ¬∑ Causal Inference ¬∑ DiD ¬∑ Propensity Score Matching ¬∑ Statistical Testing ¬∑ Gemini API ¬∑ Product Analytics</li>
    </ul>
  </section>

  <!-- USERS & METRICS -->
  <section>
    <h2>Users</h2>
    <div class="cards">
      <div class="card">
        <h3>~50,000 users</h3>
        <ul>
          <li>US-only</li>
          <li>Mixture of high-intent (organic/referral) and lower-intent (paid/affiliate) users</li>
          <li>Noticeable shift toward lower-intent cohorts after Day ~60</li>
        </ul>
      </div>
      <div class="card">
        <h3>Metrics Tracked</h3>
        <ul>
          <li><strong>North Star:</strong> Engaged Sessions per Active User (ESAU)<br><span class="metric-desc">(Sessions per day √∑ active users per day)</span></li>
          <li>Guardrails: DAU, returning user share, cohort-level ESAU splits</li>
        </ul>
      </div>
    </div>
    <div class="findings">
      <strong>Insight:</strong> This cohort shift is not obvious from aggregate dashboards but materially affects engagement.
    </div>
  </section>

  <!-- DATA SNAPSHOT -->
  <section>
    <h2>Data Snapshot</h2>
    <h3>Key Datasets</h3>
    <ul>
      <li><strong>users.csv</strong> ‚Äî signup date, acquisition channel, cohort quality</li>
      <li><strong>events.csv</strong> ‚Äî session-level engagement events</li>
      <li><strong>features.csv</strong> ‚Äî rollout timing and exposure</li>
      <li><strong>calendar_effects.csv</strong> ‚Äî seasonality multipliers</li>
    </ul>
    <div class="data-snapshot">
      <p><strong>Sample Data:</strong> User profiles (from users.csv)</p>
      <!-- Data source: product_sense/data_design/users.csv -->
      <table class="snapshot-table">
        <tr>
          <td><strong>user_id</strong></td>
          <td><strong>signup_date</strong></td>
          <td><strong>country</strong></td>
          <td><strong>cohort_quality</strong></td>
          <td><strong>acquisition_channel</strong></td>
        </tr>
        <tr><td>1</td><td>2025-10-22</td><td>US</td><td>high</td><td>referral</td></tr>
        <tr><td>2</td><td>2025-09-15</td><td>US</td><td>high</td><td>referral</td></tr>
        <tr><td>3</td><td>2025-11-11</td><td>US</td><td>low</td><td>paid</td></tr>
        <tr><td>4</td><td>2025-10-31</td><td>US</td><td>low</td><td>affiliate</td></tr>
        <tr><td>5</td><td>2025-09-21</td><td>US</td><td>high</td><td>organic</td></tr>
      </table>
    </div>
    <div class="data-snapshot">
      <p><strong>Sample Data:</strong> Feature rollout (from features.csv)</p>
      <!-- Data source: product_sense/data_design/features.csv -->
      <table class="snapshot-table">
        <tr><td><strong>feature_name</strong></td><td><strong>launch_date</strong></td><td><strong>rollout_pct</strong></td></tr>
        <tr><td>SmartFeed_v2</td><td>2025-10-26</td><td>0.6</td></tr>
      </table>
    </div>
    <div class="findings">
      <strong>Dataset Insights:</strong>
      <ul>
        <li>Engagement decay is gradual, not cliff-like</li>
        <li>Feature exposure is correlated with baseline engagement</li>
        <li>Aggregate session counts increase while ESAU declines (a classic exec trap)</li>
      </ul>
    </div>
  </section>

  <!-- APPROACH 1: NAIVE WAY -->
  <section>
    <h2>Approach 1 ‚Äî Naive Analysis</h2>
    <div class="approach">
      <h3>What I Did (High-Level)</h3>
      <ul>
        <li>Compared ESAU before vs after feature launch</li>
        <li>Ran t-tests (pre/post, exposed vs not)</li>
        <li>Applied Difference-in-Differences using exposure as treatment</li>
        <li>Observed statistically significant results</li>
      </ul>
      <!-- TODO: Insert pre vs post ESAU comparison plot -->
      <!-- TODO: Insert DiD regression table (naive) -->
    </div>
    <h3>Outcome</h3>
    <div class="metrics-box">
      <div class="metrics-row">
        <div class="metric-item">
          <div class="metric-label">ESAU Drop (Observed)</div>
          <div class="metric-value danger">-8%</div>
        </div>
        <div class="metric-item">
          <div class="metric-label">DiD Interaction Term</div>
          <div class="metric-value">Statistically Significant</div>
        </div>
      </div>
    </div>
    <div class="findings warning">
      <strong>‚ö†Ô∏è Why This Is Wrong:</strong>
      <ul>
        <li>Treated users already had higher baseline engagement</li>
        <li>Exposure was endogenous, not random</li>
        <li>Parallel trends assumption violated</li>
        <li>Post period included multiple shocks (seasonality + cohort shift)</li>
        <li>Despite ‚Äúcorrect‚Äù methods, the conclusion was misleading.</li>
      </ul>
    </div>
  </section>

  <!-- APPROACH 2: CORRECT ONE -->
  <section>
    <h2>Approach 2 ‚Äî Correct Analysis</h2>
    <div class="approach">
      <h3>What I Did (High-Level)</h3>
      <ul>
        <li>Identified endogeneity in feature exposure</li>
        <li>Built pre-treatment user features</li>
        <li>Estimated propensity scores for exposure</li>
        <li>Matched treated and control users</li>
        <li>Verified balance post-matching</li>
        <li>Re-estimated ESAU differences</li>
        <li>Ran PSM + DiD on matched samples</li>
        <li>Decomposed engagement drivers</li>
      </ul>
      <!-- TODO: Insert propensity score overlap plot (treated vs control) -->
      <!-- TODO: Insert ESAU trends after matching plot -->
    </div>
    <h3>Outcome</h3>
    <div class="metrics-box">
      <div class="metrics-row">
        <div class="metric-item">
          <div class="metric-label">Seasonality Impact</div>
          <div class="metric-value">-3%</div>
        </div>
        <div class="metric-item">
          <div class="metric-label">Cohort Shift Impact</div>
          <div class="metric-value">-4%</div>
        </div>
        <div class="metric-item">
          <div class="metric-label">Feature Bug Impact</div>
          <div class="metric-value">-1%</div>
        </div>
        <div class="metric-item">
          <div class="metric-label">True Feature Effect</div>
          <div class="metric-value positive">~0%</div>
        </div>
      </div>
    </div>
    <div class="findings">
      <strong>‚úÖ Key Insight:</strong> Baseline differences largely removed. Feature effect shrank substantially. Seasonality and cohort quality emerged as dominant drivers. Feature bug isolated to a small returning-user segment.
    </div>
  </section>

  <!-- KEY FINDINGS / DRIVER IMPACT TABLE -->
  <section>
    <h2>Key Findings</h2>
    <table class="snapshot-table">
      <tr><th>Driver</th><th>Impact on ESAU</th></tr>
      <tr><td>Seasonality / fatigue</td><td>~3%</td></tr>
      <tr><td>Cohort quality shift</td><td>~4%</td></tr>
      <tr><td>Feature interaction bug</td><td>~1%</td></tr>
      <tr><td>Total observed drop</td><td>~8%</td></tr>
    </table>
    <div class="findings">
      <strong>No single factor explains the decline; the feature is not the primary cause.</strong>
    </div>
  </section>

  <!-- WHAT WOULD HAVE GONE WRONG -->
  <section>
    <h2>What Would Have Gone Wrong Without This Analysis</h2>
    <ul>
      <li>Feature rollback would not have restored engagement</li>
      <li>Engineering effort wasted on the wrong problem</li>
      <li>Long-term roadmap disrupted</li>
      <li>Executive trust in analytics eroded</li>
      <li>Root causes (cohorts, seasonality) left unaddressed</li>
    </ul>
    <div class="findings warning">
      <strong>This is exactly how good features get killed by bad analysis.</strong>
    </div>
  </section>

  <!-- RECOMMENDATION / CONCLUSION -->
  <section>
    <h2>Recommendation & Conclusion</h2>
    <div class="recommendation positive">
      <h3>üü¢ Maintain SmartFeed_v2 rollout, fix bug</h3>
      <p>
        Maintain the SmartFeed_v2 rollout while fixing the identified returning-user interaction bug. Monitor ESAU normalized by cohort quality over the next two weeks before taking broader action.
      </p>
      <h4>Next Steps</h4>
      <ul>
        <li>Monitor ESAU and retention post-holiday</li>
        <li>Segment by cohort and acquisition channel</li>
        <li>Automate root cause analysis in the Copilot</li>
        <li>Continue using GenAI for executive summaries</li>
      </ul>
    </div>
    <div class="findings">
      <strong>Conclusion:</strong> This project demonstrates how product data science goes beyond statistical significance. By correcting for bias, validating assumptions, and framing results in decision language, analytics can prevent costly overreactions and guide leadership toward the right action.<br><br>
      <strong>üìå Final Decision:</strong> No rollback. Targeted fix + monitoring.
    </div>
    <div class="findings notes">
      <strong>Notes on Numbers:</strong><br>
      ~8% drop ‚Üí From ESAU pre/post averages (daily mean)<br>
      Seasonality (~3%) ‚Üí Compare ESAU adjusted vs unadjusted using calendar effects<br>
      Cohort shift (~4%) ‚Üí Weighted ESAU by cohort quality<br>
      Feature bug (~1%) ‚Üí Returning-user ESAU exposed vs matched control
    </div>
  </section>

  <footer>
    ¬© 2025 Abhitay Shinde | Engagement Drop Root Cause Analysis
  </footer>

</body>
</html>
