<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <meta name="description" content="Case study: Root cause analysis of engagement drop using difference-in-differences and cohort correction. Avoided unnecessary rollback through causal decomposition." />
  <title>Engagement Drop Root Cause Analysis | Case Study</title>
  <link rel="icon" type="image/svg+xml" href="favicon/favicon.svg">
  <link rel="icon" type="image/png" sizes="96x96" href="favicon/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="192x192" href="favicon/web-app-manifest-192x192.png">
  <link rel="icon" type="image/png" sizes="512x512" href="favicon/web-app-manifest-512x512.png">
  <link rel="apple-touch-icon" href="favicon/apple-touch-icon.png">
  <link rel="shortcut icon" href="favicon/favicon.ico">
  <link rel="stylesheet" href="styles/main.css">
</head>

<body>

  <aside class="side-index collapsible" aria-label="Page index">
    <button class="side-index-toggle" aria-expanded="false" aria-label="Open section list">
      <span class="side-index-icon" aria-hidden="true">
        <svg viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" aria-hidden="true">
          <path d="M3 5h14M3 10h14M3 15h14" stroke="currentColor" stroke-width="2" stroke-linecap="round"/>
        </svg>
      </span>
      <span class="side-index-label">On this page</span>
    </button>
    <div class="side-index-body" hidden>
      <div class="side-index-header">
        <div class="side-index-title">On this page</div>
        <button class="side-index-close" type="button" aria-label="Close section list">√ó</button>
      </div>
      <a href="#tldr">TL;DR</a>
      <a href="#decision-context">Decision Context</a>
      <a href="#who-we-were-comparing">Who We Compared</a>
      <a href="#esau-misleads">Why ESAU Can Mislead</a>
      <a href="#data">Data</a>
      <a href="#standard-analysis">Standard Analysis</a>
      <a href="#naive-risk">Why Naive Was Risky</a>
      <a href="#agentic-investigation">Agentic Investigation</a>
      <a href="#approach">Counterfactual Approach</a>
      <a href="#corrected-results">Corrected Results</a>
      <a href="#segments">Segment Insights</a>
      <a href="#key-findings">Key Findings</a>
    </div>
  </aside>

  <!-- NAVBAR -->
  <nav>
    <div class="left"><a href="index.html">Abhitay Shinde</a></div>
    <div class="right">
      <!-- Mobile contact icon -->
      <a
        href="index.html#contact"
        class="nav-contact-icon"
        aria-label="Contact"
        title="Contact"
      >
        üí¨
      </a>
      <button class="nav-toggle" aria-expanded="false" aria-label="Toggle navigation">‚ò∞</button>
      <div class="nav-links">
        <a href="index.html#projects">Highlights</a>
        <a href="index.html#experience">Background</a>
        <a href="index.html#contact">Contact</a>
        <a class="btn" href="Shinde Resume.pdf" target="_blank">Download Resume</a>
      </div>
    </div>
  </nav>
<section>
  <div class="breadcrumb">
    <a href="index.html">Home</a>
    <span class="crumb-sep">/</span>
    <a href="index.html#case-studies">Case Studies</a>
    <span class="crumb-sep">/</span>
    <span>Root Cause Analysis</span>
  </div>
</section>
  <!-- HERO -->
  <section class="hero">
    <h1>Was the Feed Update the Culprit? Root Cause Analysis & Targeted Fix</h1>
    <p class="subtitle">Deciding whether to rollback a major feed update or apply a focused fix that preserves long-term value.</p>
    <div class="tech-tags">
      <span class="tag">Product Analytics</span>
      <span class="tag">Causal Inference</span>
      <span class="tag">Propensity Score Matching</span>
      <span class="tag">Difference-in-Differences</span>
      <span class="tag">Agentic AI</span>
    </div>
  </section>

  <!-- TL;DR -->
   <section id="tldr">
  <section class="tldr">
    <h2>TL;DR</h2>
    <ul>
      <li>I evaluated a post-launch engagement decline following a major feed update in a consumer product.</li>
      <li>Initial analysis using aggregate metrics, t-tests, and naive Difference-in-Differences strongly suggested the feature caused the drop.</li>
      <li>Deeper analysis correcting for selection bias and cohort shifts showed the feature impact was real but not the primary driver.</li>
      <li><strong>Final decision:</strong> Do not rollback the feature. Apply a targeted fix and monitor cohort-normalized engagement.</li>
    </ul>
  </section>
  </section>
  <section id="decision-context">
    <h2>The Decision Leadership Needed</h2>
    <p>The product is a B2C, feed-based application where user engagement directly impacts retention and long-term revenue. Engagement is closely monitored by leadership and often used as an early warning signal for product regressions.</p>
    <p>Shortly after the gradual rollout of a new feed experience, leadership observed a sustained decline in engagement. The timing raised immediate concern that the feature had negatively impacted user behavior. The decision at stake was whether to rollback the feature, pause further rollout, or continue with targeted iteration.</p>
    <div class="fig">
      <img src="figures/esau_trend.png" alt="Daily ESAU trend showing post-launch decline">
      <p class="caption">Figure 1: Daily Engaged Sessions per Active User (ESAU). Engagement trends downward after feature rollout, triggering executive concern.</p>
    </div>
  </section>

<section id="who-we-were-comparing">
  <h2>Who We Were Comparing</h2>

  <p class="lead">
    The engagement drop did not affect all users equally.
    Understanding <em>who moved the metric</em> was the first priority.
  </p>


  <div class="cards">
    <div class="card">
      <h4>High-intent users</h4>
      <p>
        Returning users with strong baseline engagement and repeat usage.
      </p>
      <p class="muted">
        Small in number, disproportionate impact on ESAU.
      </p>
    </div>

    <div class="card">
      <h4>Low-intent users</h4>
      <p>
        Newly acquired or low-commitment users with volatile engagement.
      </p>
      <p class="muted">
        High volume, unstable behavior.
      </p>
    </div>

    <div class="card">
      <h4>Lifecycle stage</h4>
      <p>
        New vs returning users exhibit fundamentally different engagement dynamics.
      </p>
      <p class="muted">
        Aggregates mask these differences.
      </p>
    </div>
  </div>

  <div class="findings warning">
    <strong>Why this mattered</strong>
    <p>
      A shift in cohort mix alone can move ESAU materially,
      even if the product experience is unchanged.
      Any analysis that ignores this risks blaming the wrong cause.
    </p>
  </div>
</section>


<section id="esau-misleads">
  <h2>Why ESAU Can Mislead</h2>

  <p class="lead">
    ESAU is a useful signal, but only when the population underneath it is stable.
  </p>

  <div class="cards">
    <div class="card">
      <h4>Why ESAU is attractive</h4>
      <ul>
        <li>Normalizes for DAU growth</li>
        <li>Captures per-user engagement intensity</li>
        <li>Moves quickly after product changes</li>
      </ul>
      <p class="muted">
        Implicit assumption: the user mix is comparable over time.
      </p>
    </div>

    <div class="card">
      <h4>Where it breaks</h4>
      <ul>
        <li>Influx of low-intent users dilutes averages</li>
        <li>Returning users churn more slowly but move the metric more</li>
        <li>Lifecycle effects masquerade as product regressions</li>
      </ul>
      <p class="muted">
        The metric reacts, but not always to the right cause.
      </p>
    </div>

    <div class="card">
      <h4>How ESAU should be used</h4>
      <p>
        As a <strong>diagnostic signal</strong>, not a decision trigger.
      </p>
      <ul>
        <li>Segmented by cohort quality</li>
        <li>Anchored to returning users</li>
        <li>Interpreted alongside acquisition mix</li>
      </ul>
    </div>
  </div>

  <div class="findings warning">
    <strong>Key insight</strong>
    <p>
      ESAU was telling the truth:
      just not the truth leadership thought it was.
    </p>
  </div>
</section>


<section id="data">
  <h2>The Data Behind the Decision</h2>

  <p class="lead">
    To determine whether the feed update caused the drop,
    the analysis required visibility into <strong>baseline quality</strong>,
    <strong>feature exposure</strong>, and <strong>post-launch behavior</strong>.
  </p>

  <div class="cards">
    <div class="card">
      <h4>User-level context</h4>
      <p>
        User records capture who users were before the rollout.
      </p>
      <ul>
        <li>Signup timing and acquisition channel</li>
        <li>Cohort quality classification</li>
        <li>Returning vs new user status</li>
      </ul>
      <p class="muted">
        This answers: <em>what kind of users are we observing?</em>
      </p>
    </div>

    <div class="card">
      <h4>Behavioral outcomes</h4>
      <p>
        Event-level data tracks actual engagement over time.
      </p>
      <ul>
        <li>Session starts and counts</li>
        <li>Feature exposure flags</li>
        <li>Pre vs post indicators</li>
      </ul>
      <p class="muted">
        This answers: <em>how behavior changed after exposure</em>.
      </p>
    </div>
  </div>

  <div class="findings warning">
    <strong>Early warning signs</strong>
    <p>
      Feature exposure was correlated with baseline engagement,
      and total sessions continued to grow even as ESAU declined,
      a classic signal of cohort-driven metric movement.
    </p>
  </div>

  <p>
    Because baseline quality and post-launch behavior were observable separately,
    the analysis could isolate <em>product impact</em>
    from <em>population drift</em>.
  </p>
</section>


  <section id="standard-analysis">
  <h2>The Standard Analysis (What Most Teams Would Do)</h2>

  <p class="lead">
    The initial evidence appeared to point clearly toward a rollback.
  </p>

  <p>
    Following the conventional analytics playbook, the analysis began with
    aggregate comparisons that most teams rely on under time pressure.
  </p>

  <div class="cards">
    <div class="card">
      <h4>Step 1: Pre / post comparison</h4>
      <p>
        ESAU was compared before and after the feed rollout across the full user base.
      </p>
      <p class="muted">
        Fast, intuitive, and commonly used in production monitoring.
      </p>
    </div>

    <div class="card">
      <h4>Step 2: Treated vs control trends</h4>
      <p>
        Users exposed to the new feed were compared against non-exposed users
        over the same time window.
      </p>
      <p class="muted">
        Assumes exposure is exogenous.
      </p>
    </div>

    <div class="card">
      <h4>Step 3: Difference-in-Differences</h4>
      <p>
        A DiD model estimated the interaction between feature exposure
        and the post-launch period.
      </p>
      <p class="muted">
        Statistically rigorous, if assumptions hold.
      </p>
    </div>
  </div>

  <p class="lead">
    All three views told the same story.
  </p>

  <div class="fig">
    <img src="figures/pre_post_esau.png" alt="Pre vs post ESAU">
    <p class="caption">
      ESAU declined immediately following rollout, suggesting a negative feature impact.
    </p>
  </div>

  <div class="fig">
    <img src="figures/naive_treated_control_trend.png" alt="Naive treated vs control trend">
    <p class="caption">
      Exposed users diverged downward relative to controls, reinforcing the rollback narrative.
    </p>
  </div>

  <div class="findings danger">
    <strong>Naive conclusion</strong>
    <p>
      The feed update caused the engagement drop.
      Rolling back would restore performance.
    </p>
  </div>
</section>



  <section id="naive-risk">
    <h2>Why the Naive Conclusion Was Risky</h2>
    <div class="card findings danger">
      <strong>Core claim:</strong>
      <p>The naive analysis compared incomparable groups and attributed all change to the most visible event.</p>
      <p>The flaw was not in the math, but in the assumptions.</p>
      <p>Feature exposure was <strong>endogenous</strong>. Higher-engagement users were more likely to be exposed, and cohort composition shifted materially during the post period. At the same time, the post-launch window included seasonality and natural engagement decay.</p>
      <p>In plain terms, statistical significance masked a fundamentally biased comparison.</p>
    </div>
  </section>

<section id="agentic-investigation">
  <h2>Agentic Investigation: Structuring the Root Cause Analysis</h2>

  <p class="lead">
    Before applying deeper causal methods,
    the challenge was not computation,
    but deciding <em>what to test</em> and <em>in what order</em>
    under time pressure.
  </p>

  <p>
    To avoid anchoring on the most visible explanation (the feature),
    I designed an <strong>agentic investigation framework</strong>
    that systematically evaluated competing hypotheses
    behind the engagement decline.
  </p>

  <div class="cards">
    <div class="card">
      <h4>Hypothesis generation</h4>
      <ul>
        <li>Feature regression from the new feed</li>
        <li>Seasonality and calendar effects</li>
        <li>User quality and acquisition mix shift</li>
        <li>Instrumentation or logging issues</li>
      </ul>
      <p class="muted">
        Prevents premature fixation on a single narrative.
      </p>
    </div>

    <div class="card">
      <h4>Agent-driven test planning</h4>
      <p>
        The agent ranked hypotheses by plausibility,
        expected impact, and cost of validation,
        then selected appropriate analytical tests.
      </p>
      <ul>
        <li>Cohort stability checks</li>
        <li>Pre/post balance diagnostics</li>
        <li>Segment-level trend analysis</li>
        <li>Causal estimation where warranted</li>
      </ul>
    </div>

    <div class="card">
      <h4>Evidence synthesis</h4>
      <p>
        Results from each test were summarized
        and evaluated jointly rather than in isolation.
      </p>
      <p class="muted">
        Avoids overreacting to statistically significant
        but economically misleading signals.
      </p>
    </div>
  </div>

  <div class="findings">
    <strong>Why this mattered</strong>
    <p>
      The agentic layer ensured the analysis remained
      hypothesis-driven rather than metric-driven,
      narrowing the root cause before applying
      Difference-in-Differences and matching.
    </p>
  </div>
</section>


<section id="approach">
  <h2>My Approach: Estimating the Counterfactual</h2>

  <p class="lead">
    The critical question was not whether engagement changed,
    but what would have happened <em>without</em> the feed update.
  </p>

  <p>
    Because exposure to the feature was non-random,
    the counterfactual could not be observed directly.
    The analysis therefore focused on constructing a defensible
    approximation using only pre-rollout information.
  </p>

  <div class="cards">
    <div class="card">
      <h4>Freeze pre-treatment behavior</h4>
      <p>
        User features were engineered exclusively from behavior observed
        before the rollout.
      </p>
      <p class="muted">
        Prevents post-treatment leakage.
      </p>
    </div>

    <div class="card">
      <h4>Model exposure propensity</h4>
      <p>
        A propensity model estimated each user‚Äôs likelihood of receiving
        the new feed based on baseline engagement and lifecycle stage.
      </p>
      <p class="muted">
        Makes selection bias explicit.
      </p>
    </div>

    <div class="card">
      <h4>Match comparable users</h4>
      <p>
        Treated users were matched to control users
        with similar propensity scores.
      </p>
      <p class="muted">
        Approximates a randomized experiment.
      </p>
    </div>
  </div>

  <p>
    Engagement outcomes were then compared over the full post period,
    avoiding unstable daily ratios that exaggerate noise.
  </p>

  <p class="lead">
    Matching is only valid if treated and control users overlap.
  </p>

  <div class="fig">
    <img src="figures/propensity_overlap.png" alt="Propensity score overlap">
    <p class="caption">
      Propensity score distributions show sufficient overlap after trimming,
      validating the matching approach.
    </p>
  </div>

  <div class="findings">
    <strong>What this enabled</strong>
    <p>
      By comparing users who looked similar before exposure,
      the analysis isolated the feature‚Äôs interaction effect
      from cohort shifts and seasonality.
    </p>
  </div>
</section>



<section id="corrected-results">
  <h2>What Changed After Correcting for Bias</h2>

  <p class="lead">
    Once selection effects and cohort shifts were accounted for,
    the story changed.
  </p>

  <p>
    The naive view suggested a clear regression:
    ESAU dropped immediately after rollout.
  </p>

  <div class="findings danger">
    <strong>That conclusion was incomplete.</strong>
    <p>
      It attributed all movement to the feature,
      ignoring who entered and exited the metric.
    </p>
  </div>

  <div class="cards">
    <div class="card">
      <h4>Naive interpretation</h4>
      <ul>
        <li>~1.45% ESAU decline post-launch</li>
        <li>Statistically significant pre/post difference</li>
        <li>Treated users trend downward</li>
      </ul>
      <p class="muted">
        Comparison contaminated by selection bias.
      </p>
    </div>

    <div class="card">
      <h4>Bias-corrected view</h4>
      <ul>
        <li>~14.8% fewer sessions for treated users vs matched controls</li>
        <li>Effect real, but economically modest</li>
        <li>Insufficient to explain aggregate decline</li>
      </ul>
      <p class="muted">
        Comparison now reflects like-for-like users.
      </p>
    </div>
  </div>

  <div class="findings">
    <strong>Decision implication</strong>
    <p>
      The feature contributed marginally to the drop,
      but rolling it back would not have recovered engagement.
      The dominant drivers were cohort quality and seasonality.
    </p>
  </div>
</section>


  <section id="segments">
    <h2>Segment-Level Insights</h2>
    <div class="card">
      <p>Breaking results down by segment revealed meaningful heterogeneity:</p>
      <ul>
        <li><strong>Low-intent cohorts:</strong> engagement decline driven primarily by acquisition mix changes</li>
        <li><strong>High-intent, returning users:</strong> small but consistent interaction effect from the new feed</li>
        <li><strong>New users:</strong> minimal sensitivity to the feature</li>
      </ul>
      <p>The risk was localized friction among core users, not broad engagement collapse.</p>
    </div>
  </section>

  <section id="key-findings">
  <h2>Key Findings & Inference</h2>

  <p class="lead">
    Correcting for bias materially changed both the interpretation
    and the decision.
  </p>

  <div class="cards two-col">
    <div class="card">
      <h4>Aggregate metrics overstated feature impact</h4>
      <p>
        The observed ESAU decline was largely driven by
        cohort mix shifts and seasonality rather than
        a broad product regression.
      </p>
    </div>

    <div class="card">
      <h4>Feature exposure was non-random</h4>
      <p>
        Higher-engagement and returning users were more likely
        to receive the new feed, invalidating naive treated‚Äìcontrol
        comparisons.
      </p>
    </div>

    <div class="card">
      <h4>The causal effect existed but was modest</h4>
      <p>
        After matching, the feed update showed a real interaction
        effect, but one too small to explain the full engagement drop.
      </p>
    </div>

    <div class="card">
      <h4>Impact was concentrated in core users</h4>
      <p>
        The negative interaction primarily affected
        high-intent, returning users, while new and low-intent users
        showed minimal sensitivity.
      </p>
    </div>
  </div>

  <div class="findings">
    <strong>Inference</strong>
    <p>
      Rolling back the feature would not have restored engagement.
      The majority of the decline was structural rather than causal,
      with the feature contributing a secondary, localized effect.
    </p>
  </div>
</section>


  <div class="back-to-portfolio">
    <a href="index.html#case-studies" class="pill-link">‚Üê Back to Portfolio</a>
  </div>

  <footer>
    ¬© 2025 Abhitay Shinde
  </footer>

  <script>
    const sections = document.querySelectorAll('section[id]');
    const navLinks = document.querySelectorAll('nav .right a[href^="#"]');

    function highlightNavLink() {
      let current = '';
      sections.forEach(section => {
        const sectionTop = section.offsetTop - 100;
        const sectionHeight = section.clientHeight;
        if (window.scrollY >= sectionTop && window.scrollY < sectionTop + sectionHeight) {
          current = section.getAttribute('id');
        }
      });

      navLinks.forEach(link => {
        link.classList.remove('active');
        if (link.getAttribute('href') === `#${current}`) {
          link.classList.add('active');
        }
      });
    }

    // Add flash effect when ANY internal link is clicked
    const allInternalLinks = document.querySelectorAll('a[href^="#"]');
    allInternalLinks.forEach(link => {
      link.addEventListener('click', function(e) {
        const targetId = this.getAttribute('href').substring(1);
        const targetSection = document.getElementById(targetId);
        if (targetSection) {
          document.querySelectorAll('.highlight-flash').forEach(el => el.classList.remove('highlight-flash'));
          setTimeout(() => {
            targetSection.classList.add('highlight-flash');
            setTimeout(() => targetSection.classList.remove('highlight-flash'), 3000);
          }, 100);
        }
      });

      // Mobile toast prompt for case study links
      const caseStudyLinks = document.querySelectorAll('a[href*="feature-impact.html"], a[href*="growth-allocation.html"], a[href*="ai-insights-copilot.html"]');
      const isMobile = () => window.matchMedia('(max-width: 768px)').matches;
      let pendingHref = null;

      const ensureToast = () => {
        let overlay = document.querySelector('.toast-overlay');
        let toast = document.querySelector('.toast-banner');
        if (!overlay || !toast) {
          overlay = document.createElement('div');
          overlay.className = 'toast-overlay';
          toast = document.createElement('div');
          toast.className = 'toast-banner';
          toast.innerHTML = `
            <div class="toast-title">Best viewed on desktop</div>
            <div class="toast-text">These case studies are easier to read on a larger screen. Continue on mobile?</div>
            <div class="toast-buttons">
              <button class="btn-secondary" type="button" data-toast-cancel>Cancel</button>
              <button class="btn-primary" type="button" data-toast-continue>Continue</button>
            </div>
          `;
          overlay.appendChild(toast);
          document.body.appendChild(overlay);

          const closeToast = () => {
            overlay?.classList.remove('show');
            toast?.classList.remove('show');
            pendingHref = null;
          };

          overlay.addEventListener('click', (e) => {
            if (e.target === overlay) closeToast();
          });

          toast.querySelector('[data-toast-cancel]')?.addEventListener('click', closeToast);
          toast.querySelector('[data-toast-continue]')?.addEventListener('click', () => {
            if (pendingHref) {
              window.location.href = pendingHref;
            }
            closeToast();
          });
        }
        return { overlay, toast };
      };

      caseStudyLinks.forEach(link => {
        link.addEventListener('click', (e) => {
          if (!isMobile()) return;
          e.preventDefault();
          pendingHref = link.href;
          const { overlay, toast } = ensureToast();
          overlay.classList.add('show');
          toast.classList.add('show');
        });
      });
    });

    // Mobile nav toggle
    const navRight = document.querySelector('nav .right');
    const navToggle = document.querySelector('.nav-toggle');
    const navLinksWrap = document.querySelector('.nav-links');
    if (navRight && navToggle && navLinksWrap) {
      navToggle.addEventListener('click', () => {
        const isOpen = navRight.classList.toggle('open');
        navToggle.setAttribute('aria-expanded', String(isOpen));
      });
      navLinksWrap.querySelectorAll('a').forEach(a => {
        a.addEventListener('click', () => {
          navRight.classList.remove('open');
          navToggle.setAttribute('aria-expanded', 'false');
        });
      });
    }

    // Side index toggle (icon on mobile, hover/click on desktop)
    const sideIndex = document.querySelector('.side-index.collapsible');
    const sideToggle = sideIndex?.querySelector('.side-index-toggle');
    const sideBody = sideIndex?.querySelector('.side-index-body');
    const sideClose = sideIndex?.querySelector('.side-index-close');
    const sideMedia = window.matchMedia('(max-width: 768px)');

    if (sideIndex && sideToggle && sideBody) {
      const openPanel = () => {
        sideIndex.classList.add('open');
        sideToggle.setAttribute('aria-expanded', 'true');
        sideBody.hidden = false;
      };

      const closePanel = () => {
        sideIndex.classList.remove('open');
        sideToggle.setAttribute('aria-expanded', 'false');
        if (sideMedia.matches) {
          setTimeout(() => {
            if (!sideIndex.classList.contains('open')) {
              sideBody.hidden = true;
            }
          }, 180);
        } else {
          sideBody.hidden = true;
        }
      };

      sideToggle.addEventListener('click', (e) => {
        e.preventDefault();
        if (sideIndex.classList.contains('open')) {
          closePanel();
        } else {
          openPanel();
        }
      });

      sideClose?.addEventListener('click', closePanel);

      sideIndex.addEventListener('mouseenter', () => {
        if (!sideMedia.matches) {
          openPanel();
        }
      });

      sideIndex.addEventListener('mouseleave', () => {
        if (!sideMedia.matches) {
          closePanel();
        }
      });

      sideMedia.addEventListener('change', () => {
        sideBody.hidden = !sideIndex.classList.contains('open');
      });
    }

    window.addEventListener('scroll', highlightNavLink);
    highlightNavLink();
  </script>

</body>
</html>
