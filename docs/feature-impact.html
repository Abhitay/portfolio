<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <meta name="description" content="Case study: Using propensity score matching to evaluate a feed feature's true impact on retention. How selection bias masked zero effect in a 110-140% engagement lift." />
  <title>Feature Impact Analysis | Causal Inference Case Study</title>
  <link rel="icon" type="image/x-icon" href="favicon.ico">
  <link rel="stylesheet" href="styles/main.css">
</head>

<body>

  <aside class="side-index collapsible" aria-label="Page index">
    <button class="side-index-toggle" aria-expanded="false">On this page</button>
    <div class="side-index-body" hidden>
      <div class="side-index-title">On this page</div>
      <a href="#tldr">TL;DR</a>
      <a href="#decision-context">Decision Context</a>
      <a href="#who-we-were-comparing">Who We Compared</a>
      <a href="#misleading-metrics">Why Standard Metrics Misled</a>
      <a href="#data">Data</a>
      <a href="#naive-comparison">Naive Comparison</a>
      <a href="#naive-failure">Why It Failed</a>
      <a href="#method">Method</a>
      <a href="#results">Results</a>
      <a href="#recommendation">Recommendation</a>
    </div>
  </aside>

  <!-- NAVBAR -->
  <nav>
    <div class="left"><a href="index.html">Abhitay Shinde</a></div>
    <div class="right">
      <button class="nav-toggle" aria-expanded="false" aria-label="Toggle navigation">☰</button>
      <div class="nav-links">
        <a href="index.html#projects">Highlights</a>
        <a href="index.html#experience">Background</a>
        <a href="index.html#contact">Contact</a>
        <a class="btn" href="Shinde Resume.pdf" target="_blank">Download Resume</a>
      </div>
    </div>
  </nav>

  <section>
    <div class="breadcrumb">
      <a href="index.html">Home</a>
      <span class="crumb-sep">/</span>
      <a href="index.html#case-studies">Case Studies</a>
      <span class="crumb-sep">/</span>
      <span>Feature Impact Analysis</span>
    </div>
  </section>

  <!-- HERO -->
  <section class="hero">
    <h1>When Engagement Lifts Mislead: Did the New Feed Improve Retention?</h1>
    <p class="subtitle">
      Why the initial engagement signal mattered, and how causal correction changed the decision.
    </p>
    <div class="tech-tags">
      <span class="tag">Product Analytics</span>
      <span class="tag">Causal Inference</span>
      <span class="tag">Propensity Score Matching</span>
      <span class="tag">Logistic Regression</span>
    </div>
  </section>

  <!-- TLDR -->
   <section id="tldr">
  <section class="tldr">
    <h2>TL;DR</h2>
    <p>
      A new feed iteration appeared to drive a <strong>~110–140% lift in engagement</strong>.
      After correcting for selection bias using <strong>propensity score matching</strong>,
      I found <strong>no meaningful improvement in 7-day retention</strong>.
      The lift was driven by <em>who</em> received the feature, not the feature itself.
    </p>
    <p><strong>Final decision: do not ship the feature.</strong></p>
  </section></section>
  <!-- CONTEXT -->
  <section id="decision-context">
    <h2>The Decision That Triggered This Analysis</h2>
    <p>
      This case study evaluates a feed-ranking change in a generic consumer social app.
      The feed is a core surface for engagement and long-term retention, making even small
      ranking changes high-risk.
    </p>
    <p>
      The feature introduced more aggressive personalization based on predicted relevance.
      Exposure was <strong>not randomized</strong>, making this an observational analysis
      rather than a clean A/B test.
    </p>
  </section>

  <!-- WHO WE WERE COMPARING -->
  <section id="who-we-were-comparing">
    <h2>Who We Were Comparing</h2>
    <p class="section-note">
      Users clustered naturally into three behavioral segments.
    </p>

    <div class="split">
      <div>
        <div class="cards">
          <div class="card compact">
            <h4>Low-engagement users</h4>
            <p>Infrequent usage, low baseline interaction</p>
          </div>
          <div class="card compact">
            <h4>Normal users</h4>
            <p>Moderate engagement, majority of the base</p>
          </div>
          <div class="card compact">
            <h4>Power users</h4>
            <p>Highly engaged, critical to platform health</p>
          </div>
        </div>

        <p style="margin-top:16px;">
          These segments matter because engagement propensity differs sharply across them.
        </p>
      </div>

      <div class="card figure-only">
        <div class="fig">
          <img src="figures/1a.png" alt="User Segment Distribution">
        </div>
      </div>
    </div>
  </section>

  <!-- METRICS -->
<section id="misleading-metrics">
  <h2>Why Standard Metrics Were Misleading</h2>
  <p class="lead">
    Engagement spikes are diagnostic, not decisive. Success criteria were defined upfront.
  </p>

  <div class="two-col metrics-contrast">

    <!-- NORTH STAR -->
    <div class="card metric-primary">
      <span class="metric-label">North Star Metric</span>
      <h3>7-day retention</h3>
      <p class="muted">
        Best proxy for habit formation and long-term value.
      </p>
    </div>

    <!-- DIAGNOSTICS -->
    <div class="card metric-secondary">
      <span class="metric-label">Diagnostic Metrics</span>
      <ul class="clean-list">
        <li>Cards viewed per session</li>
        <li>Bounce rate</li>
      </ul>
      <p class="muted">
        Useful for understanding behavior, not for making ship decisions.
      </p>
    </div>

  </div>

  <div class="findings">
    <strong>Decision rule</strong>
    <p>
      The decision hinged on whether users returned, not whether they interacted more in a single session.
    </p>
  </div>
</section>



  <!-- DATA -->
<section id="data">
  <h2>The Data Behind the Decision</h2>

  <p class="lead">
    The analysis relied on two complementary datasets that together made it possible
    to separate <strong>user quality</strong> from <strong>feature impact</strong>.
  </p>

  <!-- DATA OVERVIEW -->
  <div class="two-col data-sources">

    <!-- USERS TABLE -->
    <div class="card">
      <span class="metric-label">Users Table</span>
      <p class="muted">
        One row per user. Used to model selection bias and long-term outcomes.
      </p>
      <ul class="clean-list">
        <li>User segment (low / normal / power)</li>
        <li>Baseline engagement score (pre-exposure)</li>
        <li>Feature exposure flag</li>
        <li>7-day retention outcome</li>
      </ul>
    </div>

    <!-- EVENTS TABLE -->
    <div class="card">
      <span class="metric-label">Events Table</span>
      <p class="muted">
        Session-level behavioral data. Used to measure short-term engagement.
      </p>
      <ul class="clean-list">
        <li>Session start / end events</li>
        <li>Card view events per session</li>
        <li>Feature flag at time of interaction</li>
      </ul>
    </div>

  </div>

  <!-- WHY THIS DATA WORKS -->
  <div class="cards">
    <div class="card">
      <h4>Unit of analysis</h4>
      <p>
        Users for retention analysis; sessions for engagement diagnostics.
      </p>
    </div>

    <div class="card">
      <h4>Time window</h4>
      <p>
        Baseline behavior measured pre-exposure, outcomes tracked over 7 days post-exposure.
      </p>
    </div>

    <div class="card">
      <h4>Why this matters</h4>
      <p>
        Separating baseline behavior from outcomes makes causal adjustment possible.
      </p>
    </div>
  </div>

  <!-- KEY SIGNAL -->
  <div class="findings warning">
    <strong>Key data signal</strong>
    <p>
      Users exposed to the new feature had systematically higher baseline engagement,
      indicating strong selection bias in feature exposure.
    </p>
  </div>

  <p class="lead">
    This meant naive engagement comparisons primarily reflected
    <strong>who received the feature</strong>,
    not <strong>what the feature caused</strong>.
  </p>
</section>



 <section id="naive-comparison">
  <h2>The Naive Comparison (Why It Deceived Us)</h2>
  <p class="lead">A dashboard view suggested a dramatic engagement win.</p>

  <div class="two-col">

    <!-- LEFT COLUMN -->
    <div class="stack">

      <!-- Metrics card -->
      <div class="card">
        <h4>Average cards viewed per session</h4>

        <div class="metric-row">
          <span>Control users</span>
          <strong>~2.3</strong>
        </div>

        <div class="metric-row">
          <span>New feature users</span>
          <strong>~4.9</strong>
        </div>

        <hr />

        <div class="metric-row highlight-row">
          <span>Observed lift</span>
          <strong class="negative">~110–140%</strong>
        </div>
      </div>

      <!-- Interpretation card -->
      <div class="card subtle">
        <strong>Why this looked convincing</strong>
        <p>
          At face value, this appears to be a major engagement win.
          A standard dashboard would strongly suggest shipping.
        </p>
      </div>

    </div>

    <!-- RIGHT COLUMN -->
    <div class="card figure-only">
      <div class="fig">
        <img src="figures/1b.png" alt="Naive Engagement Comparison">
      </div>
      <p class="caption">
        Takeaway: the lift conflates user quality with feature impact.
      </p>
    </div>

  </div>
</section>


  <!-- FAILURE -->
<section id="naive-failure">
  <h2>Why the Naive Comparison Failed</h2>

  <div class="findings danger">
    <strong>Core issue</strong>
    <p>
      Exposure to the new feed was non-random and strongly correlated with baseline engagement.
    </p>
  </div>

  <p class="lead">
    The comparison measured <strong>high-engagement users vs low-engagement users</strong>,
    not the causal effect of the feature.
  </p>

  <p class="muted">
  Analogy: comparing basketball players to accountants and concluding basketball makes people taller.
</p>

</section>


  <!-- METHOD -->
  <section id="method">
    <h2>How I Isolated the Causal Effect</h2>
    <p>
      I applied propensity score matching to approximate a randomized experiment.
    </p>

    <div class="cards three-col">
      <div class="card compact">
        <h4>Non-random treatment</h4>
        <p>Assignment depended on behavior.</p>
      </div>
      <div class="card compact">
        <h4>Observable confounders</h4>
        <p>Segment and baseline engagement.</p>
      </div>
      <div class="card compact">
        <h4>RCT approximation</h4>
        <p>Matched treated and control users.</p>
      </div>
    </div>
  </section>

  <!-- RESULTS -->
  <section id="results">
  <h2>What Changed After Correcting for Bias</h2>
  <p class="lead">
    Matching shifted the comparison onto like-for-like users.
  </p>

  <div class="two-col">

    <!-- LEFT COLUMN -->
    <div class="stack">

      <!-- Metrics card -->
      <div class="card">
        <h4>7-day retention after adjustment</h4>

        <div class="metric-row">
          <span>New feature retention</span>
          <strong>~71%</strong>
        </div>

        <div class="metric-row">
          <span>Control retention</span>
          <strong>~73%</strong>
        </div>

        <hr />

        <div class="metric-row highlight-row">
          <span>Treatment effect</span>
          <strong class="neutral">≈ 0</strong>
        </div>
      </div>

      <!-- Interpretation card -->
      <div class="card subtle positive">
        <strong>What this tells us</strong>
        <p>
          Once comparable users are evaluated, the apparent engagement win
          disappears. The feature does not improve retention.
        </p>
      </div>

    </div>

    <!-- RIGHT COLUMN -->
    <div class="card figure-only">
      <div class="fig">
        <img src="figures/1d.png" alt="7-Day Retention After Propensity Score Matching">
      </div>
      <p class="caption">
        Retention remains unchanged after causal adjustment.
      </p>
    </div>

  </div>
</section>

  <!-- RECOMMENDATION -->
  <section id="recommendation">
    <h2>Recommendation</h2>
    <div class="findings danger">
      <strong>Do not ship the feature.</strong>
      The feature failed to improve the north-star metric and introduces risk.
    </div>
  </section>

  <div class="back-to-portfolio">
    <a href="index.html#case-studies" class="pill-link">← Back to Portfolio</a>
  </div>

  <footer>
    © 2025 Abhitay Shinde
  </footer>

  <script>
    const sections = document.querySelectorAll('section[id]');
    const navLinks = document.querySelectorAll('nav .right a[href^="#"]');

    function highlightNavLink() {
      let current = '';
      sections.forEach(section => {
        const sectionTop = section.offsetTop - 100;
        const sectionHeight = section.clientHeight;
        if (window.scrollY >= sectionTop && window.scrollY < sectionTop + sectionHeight) {
          current = section.getAttribute('id');
        }
      });

      navLinks.forEach(link => {
        link.classList.remove('active');
        if (link.getAttribute('href') === `#${current}`) {
          link.classList.add('active');
        }
      });
    }

    // Add flash effect when ANY internal link is clicked
    const allInternalLinks = document.querySelectorAll('a[href^="#"]');
    allInternalLinks.forEach(link => {
      link.addEventListener('click', function(e) {
        const targetId = this.getAttribute('href').substring(1);
        const targetSection = document.getElementById(targetId);
        if (targetSection) {
          document.querySelectorAll('.highlight-flash').forEach(el => el.classList.remove('highlight-flash'));
          setTimeout(() => {
            targetSection.classList.add('highlight-flash');
            setTimeout(() => targetSection.classList.remove('highlight-flash'), 3000);
          }, 100);
        }
      });

      // Mobile toast prompt for case study links (feature-impact only)
      const caseStudyLinks = document.querySelectorAll('a[href*="feature-impact.html"], a[href*="growth-allocation.html"], a[href*="ai-insights-copilot.html"]');
      const isMobile = () => window.matchMedia('(max-width: 768px)').matches;
      let pendingHref = null;

      const ensureToast = () => {
        let toast = document.querySelector('.toast-banner');
        if (!toast) {
          toast = document.createElement('div');
          toast.className = 'toast-banner';
          toast.innerHTML = `
            <div class="toast-title">Best viewed on desktop</div>
            <div class="toast-text">These case studies are easier to read on a larger screen. Continue on mobile?</div>
            <div class="toast-buttons">
              <button class="btn-secondary" type="button" data-toast-cancel>Cancel</button>
              <button class="btn-primary" type="button" data-toast-continue>Continue</button>
            </div>
          `;
          document.body.appendChild(toast);

          const closeToast = () => {
            toast.classList.remove('show');
            pendingHref = null;
          };

          toast.querySelector('[data-toast-cancel]')?.addEventListener('click', closeToast);
          toast.querySelector('[data-toast-continue]')?.addEventListener('click', () => {
            if (pendingHref) {
              window.location.href = pendingHref;
            }
            closeToast();
          });
        }
        return toast;
      };

      caseStudyLinks.forEach(link => {
        link.addEventListener('click', (e) => {
          if (!isMobile()) return;
          e.preventDefault();
          pendingHref = link.href;
          const toast = ensureToast();
          toast.classList.add('show');
        });
      });
    });

    // Mobile nav toggle
    const navRight = document.querySelector('nav .right');
    const navToggle = document.querySelector('.nav-toggle');
    const navLinksWrap = document.querySelector('.nav-links');
    if (navRight && navToggle && navLinksWrap) {
      navToggle.addEventListener('click', () => {
        const isOpen = navRight.classList.toggle('open');
        navToggle.setAttribute('aria-expanded', String(isOpen));
      });
      navLinksWrap.querySelectorAll('a').forEach(a => {
        a.addEventListener('click', () => {
          navRight.classList.remove('open');
          navToggle.setAttribute('aria-expanded', 'false');
        });
      });
    }

    // Collapsible side index (hover)
    const sideIndex = document.querySelector('.side-index.collapsible');
    const sideToggle = sideIndex?.querySelector('.side-index-toggle');
    const sideBody = sideIndex?.querySelector('.side-index-body');
    if (sideIndex && sideToggle && sideBody) {
      const openPanel = () => {
        sideIndex.classList.add('open');
        sideToggle.setAttribute('aria-expanded', 'true');
        sideBody.hidden = false;
      };
      const closePanel = () => {
        sideIndex.classList.remove('open');
        sideToggle.setAttribute('aria-expanded', 'false');
        sideBody.hidden = true;
      };
      sideIndex.addEventListener('mouseenter', openPanel);
      sideIndex.addEventListener('mouseleave', closePanel);
    }

    window.addEventListener('scroll', highlightNavLink);
    highlightNavLink();
  </script>

</body>
</html>
