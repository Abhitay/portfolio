<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Feature Impact & Experimentation Platform | Case Study</title>

  <link rel="stylesheet" href="styles/main.css">

  <style>
    /* ---------- PAGE-SPECIFIC STYLES: feature-impact.html ---------- */

    section {
      padding: 5px 16px;
      max-width: 950px;
      margin: 0 auto;
    }

    section + section {
      padding-top: 0;
    }

    /* ---------- CODE/PSEUDOCODE ---------- */
    .code-block {
      background: #1f2937;
      color: #f3f4f6;
      padding: 14px;
      border-radius: 6px;
      overflow-x: auto;
      font-family: 'Courier New', monospace;
      font-size: 12px;
      line-height: 1.4;
      margin: 16px 0;
      display: none;
    }

    .code-label {
      font-size: 12px;
      color: #9ca3af;
      margin-bottom: 8px;
      font-weight: 500;
    }

    /* ---------- METRICS BOX ---------- */
    .metrics-box {
      background: var(--card);
      border: 1px solid var(--border);
      border-radius: 8px;
      padding: 24px;
      margin: 24px 0;
    }

    .metrics-row {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
      gap: 20px;
      margin-top: 12px;
    }

    .metric-item {
      background: white;
      padding: 16px;
      border-radius: 6px;
      border: 1px solid var(--border);
    }

    .metric-label {
      font-size: 12px;
      color: var(--muted);
      margin-bottom: 6px;
      text-transform: uppercase;
      letter-spacing: 0.5px;
    }

    .metric-value {
      font-size: 24px;
      font-weight: 600;
      color: var(--text);
    }

    /* ---------- APPROACH SECTION ---------- */
    .approach {
      background: var(--card);
      border: 1px solid var(--border);
      border-radius: 12px;
      padding: 28px;
      margin: 28px 0;
    }

    .approach h3 {
      margin-top: 0;
    }

    .approach-step {
      margin: 16px 0;
      padding-left: 24px;
      border-left: 3px solid var(--accent);
    }

    .approach-step strong {
      color: var(--text);
    }

    /* ---------- FINDINGS BOX ---------- */
    

    /* ---------- DATA SNAPSHOT ---------- */
    .data-snapshot {
      background: var(--card);
      border: 1px solid var(--border);
      border-radius: 8px;
      padding: 20px;
      margin: 20px 0;
    }

    .snapshot-table {
      font-size: 13px;
      width: 100%;
      border-collapse: collapse;
    }

    .snapshot-table td {
      padding: 8px;
      border-bottom: 1px solid var(--border);
    }

    .snapshot-table tr:last-child td {
      border-bottom: none;
    }

    /* ---------- RECOMMENDATION ---------- */
    .recommendation {
      background: #fee2e2;
      border: 2px solid var(--danger);
      border-radius: 8px;
      padding: 24px;
      margin: 28px 0;
    }

    .recommendation h3 {
      color: var(--danger);
      margin-top: 0;
    }

    .recommendation.positive {
      background: #f0fdf4;
      border-color: var(--success);
    }

    .recommendation.positive h3 {
      color: var(--success);
    }

    /* ---------- BACK LINK ---------- */
    .back-link {
      font-size: 14px;
      margin-bottom: 20px;
    }
  </style>
</head>

<body>

  <!-- NAVBAR -->
  <nav>
  <div class="left">Abhitay Shinde</div>
  <div class="right">
    <a href="index.html#projects">Projects</a>
    <a href="index.html#experience">Background</a>
    <a href="index.html#contact">Contact</a>
    <a class="btn" href="../resume.pdf" target="_blank">Download Resume</a>
  </div>
</nav>


  <!-- HERO -->
  <section class="hero">
    <h1>Feature Impact & Experimentation Platform</h1>
    <div class="tech-tags">
  <span class="tag">Product Analytics</span>
  <span class="tag">Causal Inference</span>
  <span class="tag">Propensity Score Matching</span>
  <span class="tag">T-Test</span>
  <span class="tag">Logistic Regression</span>
</div>

    <p>
      A comprehensive case study in causal inference: How propensity score matching uncovered 
      a false positive signal in product metrics and prevented a risky feature rollout.
    </p>
    
  </section>
<section>
  <div class="recommendation danger">
    <h3>TL;DR ‚Äî Product Decision</h3>
    <ul>
      <li>‚ùå No meaningful retention impact after causal correction</li>
      <li>‚ö†Ô∏è Engagement lift driven by selection bias</li>
      <li>üö® Power users at risk</li>
    </ul>
    <p><strong>Decision:</strong> Roll back SmartFeed_v2 and work on creating a balanced recommender</p>
  </div>
</section>
  <!-- PRODUCT DETAILS -->
  <section>
    <h2>Product Details</h2>
    
    <h3>What is SmartFeed?</h3>
    <p>
      SmartFeed helps users discover relevant content quickly. Users browse short summaries of events and can view detailed content cards they're interested in. It's a core feature for user engagement.
    </p>

    <h3>SmartFeed_v2: Personalized Ranking</h3>
    <p>
      We tested a new version that <strong>reorders content cards based on relevance</strong>. The idea: showing more relevant content first should increase engagement and keep users coming back. But did it actually work?
    </p>

    <h3>What We Measured</h3>
    <ul>
      <li><strong>Primary Goal:</strong> 7-day retention ‚Äî will users come back?</li>
      <li><strong>Secondary:</strong> Cards viewed per session, bounce rate</li>
    </ul>
  </section>

  <!-- USERS & USAGE -->
  <section>
    <h2>The Users We Tested</h2>

    <p>
      We tested SmartFeed_v2 with three types of users. Some got the personalized version, others used the original. Here's what we found:
    </p>
<div class="cards">
    <div class="card user new">
      <div class="user-header">
        <span class="emoji">üå±</span><h3>New Users</h3>
      </div>
      <p class="user-subtitle">First-time users forming habits.</p>
      <ul>
        <li>Low baseline engagement</li>
        <li>High onboarding sensitivity</li>
        <li>Retention driven by clarity</li>
      </ul>
    </div>

    <div class="card user returning">
      <div class="user-header">
        <span class="emoji">üîÅ</span><h3>Returning Users</h3>
      </div>
      <p class="user-subtitle">Steady repeat users.</p>
      <ul>
        <li>Moderate engagement history</li>
        <li>Expect consistency</li>
        <li>Retention reflects value</li>
      </ul>
    </div>

    <div class="card user power">
      <div class="user-header">
        <span class="emoji">‚ö°</span><h3>Power Users</h3>
      </div>
      <p class="user-subtitle">Highly engaged, high impact.</p>
      <ul>
        <li>High baseline engagement</li>
        <li>Value discovery & control</li>
        <li>Most sensitive to regressions</li>
      </ul>
    </div>
  </div>

  <!-- DATA SNAPSHOT -->
  <section>
    <h3>Data Snapshot</h3>
    <p>Analysis conducted on a cohort of 13 users over a 3-day period (Sept 7-9, 2024).</p>

    <h4>Users Dataset</h4>
    <div class="data-snapshot">
      <p><strong>Sample Data:</strong> User profiles with engagement baseline and feature flag assignment</p>
      <table class="snapshot-table">
        <tr>
          <td><strong>user_id</strong></td>
          <td><strong>signup_date</strong></td>
          <td><strong>engagement_score</strong></td>
          <td><strong>segment</strong></td>
          <td><strong>feature_flag</strong></td>
        </tr>
        <tr>
          <td>c6a10605-f4bb-4247-a4b8-47a3e4dd2d25</td>
          <td>2024-09-07</td>
          <td>0.264</td>
          <td>new</td>
          <td>control</td>
        </tr>
        <tr>
          <td>f7b1849e-f477-45c6-98e2-b1df3848b346</td>
          <td>2024-09-04</td>
          <td>0.156</td>
          <td>new</td>
          <td>control</td>
        </tr>
        <tr>
          <td>0e3eff2c-dc61-40a1-9c80-1a2cfaeb2b85</td>
          <td>2024-09-08</td>
          <td>0.360</td>
          <td>returning</td>
          <td>SmartFeed_v2</td>
        </tr>
        <tr>
          <td>0a2845bd-ae7f-45b9-8182-fdf0b43c5160</td>
          <td>2024-09-05</td>
          <td>0.118</td>
          <td>new</td>
          <td>control</td>
        </tr>
        <tr>
          <td>77f1e5cc-e90a-46ed-9a02-dcd11759418a</td>
          <td>2024-09-07</td>
          <td>0.392</td>
          <td>returning</td>
          <td>SmartFeed_v2</td>
        </tr>
      </table>
    </div>

    <h4>Events Dataset Sample</h4>
    <div class="data-snapshot">
      <p><strong>Sample Data:</strong> User session events showing activity and feature assignment at exposure time</p>
      <table class="snapshot-table">
        <tr>
          <td><strong>user_id</strong></td>
          <td><strong>event_time</strong></td>
          <td><strong>event_name</strong></td>
          <td><strong>session_id</strong></td>
          <td><strong>feature_flag</strong></td>
        </tr>
        <tr>
          <td>0e3eff2c-dc61-40a1-9c80-1a2cfaeb2b85</td>
          <td>2024-09-08 00:03:00</td>
          <td>card_view</td>
          <td>64290d26-ff75-4018-ac47-231c7deba28e</td>
          <td>SmartFeed_v2</td>
        </tr>
        <tr>
          <td>0e3eff2c-dc61-40a1-9c80-1a2cfaeb2b85</td>
          <td>2024-09-08 00:35:00</td>
          <td>session_end</td>
          <td>64290d26-ff75-4018-ac47-231c7deba28e</td>
          <td>SmartFeed_v2</td>
        </tr>
        <tr>
          <td>77f1e5cc-e90a-46ed-9a02-dcd11759418a</td>
          <td>2024-09-09 00:00:00</td>
          <td>session_start</td>
          <td>e036fdf1-93c5-4bef-b7f0-41b721e53271</td>
          <td>SmartFeed_v2</td>
        </tr>
        <tr>
          <td>77f1e5cc-e90a-46ed-9a02-dcd11759418a</td>
          <td>2024-09-09 00:17:00</td>
          <td>card_view</td>
          <td>e036fdf1-93c5-4bef-b7f0-41b721e53271</td>
          <td>SmartFeed_v2</td>
        </tr>
        <tr>
          <td>77f1e5cc-e90a-46ed-9a02-dcd11759418a</td>
          <td>2024-09-09 00:15:00</td>
          <td>card_view</td>
          <td>e036fdf1-93c5-4bef-b7f0-41b721e53271</td>
          <td>SmartFeed_v2</td>
        </tr>
      </table>
    </div>

    <h4>Key Dataset Characteristics</h4>
    <ul>
      <li><strong>Sample Size:</strong> 13 users, 20 events across 3-day period</li>
      <li><strong>Treatment Assignment:</strong> 5 users in SmartFeed_v2, 8 in control</li>
      <li><strong>Engagement Variance:</strong> Engagement scores range from 0.12 to 0.61</li>
      <li><strong>Selection Bias Risk:</strong> Users with higher engagement scores are overrepresented in treatment group</li>
    </ul>
  </section>

  <!-- APPROACH 1: NAIVE BAYES -->
  <section>
    <h2>Approach 1: Simple Comparison (The Wrong Way)</h2>

    <p>
      First, we did the simplest thing: compare average cards viewed for users with the new feature vs. the old feature. This sounds straightforward, right?
    </p>

    <h3>What We Found</h3>
    <div class="metrics-box">
      <div class="metrics-row">
        <div class="metric-item">
          <div class="metric-label">New Feature Users (Avg)</div>
          <div class="metric-value positive">5.2 cards/session</div>
        </div>
        <div class="metric-item">
          <div class="metric-label">Old Feature Users (Avg)</div>
          <div class="metric-value">2.1 cards/session</div>
        </div>
        <div class="metric-item">
          <div class="metric-label">Difference</div>
          <div class="metric-value positive">+144%</div>
        </div>
      </div>
    </div>

    <div class="findings warning">
      <strong>‚ö†Ô∏è Our Initial Reaction:</strong> "Wow! Users with the new feature view 2.5x more cards. This is amazing‚Äîship it!"
    </div>

    <h3>Why This Analysis Was Wrong</h3>
    <p>
      Here's the problem: <strong>different types of users got different features</strong>. More engaged users naturally prefer new, personalized content. So we're comparing:
    </p>
    <ul>
      <li>Super engaged users (mostly got the new feature)</li>
      <li>Less engaged users (mostly got the old feature)</li>
    </ul>

    <p>
      The higher engagement might just be because <em>those users are more engaged by default</em>‚Äînot because the feature is better. It's like comparing height between basketball players and accountants‚Äîof course basketball players are taller, but it's not because of their job!
    </p>

  <!-- APPROACH 2: PSM -->
  <section>
    <h2>Approach 2: Improved Analysis (Propensity Score Matching)</h2>

    <div class="approach">
      <h3>Methodology</h3>
      <p>
        <strong>Type:</strong> Propensity Score Matching (PSM) with logistic regression<br>
        <strong>Key Innovation:</strong> Match treatment and control users based on propensity to be exposed to SmartFeed_v2<br>
        <strong>Metric:</strong> 7-day retention rate (north star)<br>
        <strong>Approach:</strong> Create balanced treatment and control groups, then compare retention outcomes
      </p>

      <h3>What is Propensity Score Matching?</h3>
      <p>
        PSM is a technique for causal inference from observational data. The core idea: 
        <strong>users self-select into treatment (SmartFeed_v2) based on observable characteristics.</strong> 
        By matching treated users with similar control users, we create two groups that appear 
        randomly assigned, enabling valid causal comparison.
      </p>

      <h3>The Analysis Steps</h3>
      <div class="approach-step">
        <strong>Step 1: Measure Exposure & Outcome Timing</strong>
        <p>
          Identify each user's first session (exposure time). Track retention 6-8 days later 
          (7-day mark). Measure retention as binary: Did user have a session in that window?
        </p>
      </div>
      <div class="approach-step">
        <strong>Step 2: Build Propensity Score Model</strong>
        <p>
          Fit logistic regression: Predict probability user is assigned to SmartFeed_v2 
          using user segment + baseline engagement score as features.
        </p>
      </div>
      <div class="approach-step">
        <strong>Step 3: Match Users on Propensity</strong>
        <p>
          For each treated user, find nearest control user with similar propensity score. 
          This creates matched pairs where treatment assignment appears random (conditional on propensity).
        </p>
      </div>
      <div class="approach-step">
        <strong>Step 4: Compare Retention in Matched Cohort</strong>
        <p>
          Calculate retention rates in matched cohorts. The difference now isolates 
          the causal effect of SmartFeed_v2 on retention, unconfounded by user characteristics.
        </p>
      </div>
    </div>

    <h3>Pseudocode for PSM</h3>
    <div class="code-block">
      <div class="code-label">PSEUDOCODE: Propensity Score Matching</div>
      <pre>Algorithm: PropensityScoreMatching(users, events)
  
  // Step 1: Build propensity score model
  for each user:
    exposure_time = first_session_time
    features = [user.segment_encoded, user.engagement_score]
    treatment_label = (user.feature_flag == "SmartFeed_v2") ? 1 : 0
  
  propensity_model = LogisticRegression(features, treatment_label)
  
  for each user:
    user.propensity_score = propensity_model.predict_proba(user.features)
  
  // Step 2: Measure retention outcomes
  for each user:
    retention_window_start = exposure_time + 6 days
    retention_window_end = exposure_time + 8 days
    user.retained = count(sessions in [start, end]) > 0 ? 1 : 0
  
  // Step 3: Match treated to control on propensity
  treated_users = filter users where feature_flag == "SmartFeed_v2"
  control_users = filter users where feature_flag == "control"
  
  matched_pairs = []
  for each treated_user:
    // Find closest control user by propensity score
    closest_control = min(control_users, 
      distance=|treated.propensity - control.propensity|)
    matched_pairs.append((treated_user, closest_control))
  
  // Step 4: Compare retention in matched cohort
  treatment_retention_rate = average(retained for treated in matched_pairs)
  control_retention_rate = average(retained for control in matched_pairs)
  
  treatment_effect = treatment_retention_rate - control_retention_rate
  
  return {
    treatment_retention_rate,
    control_retention_rate, 
    treatment_effect,
    matched_pairs
  }</pre>
    </div>

    <h3>Key Findings</h3>
    <div class="metrics-box">
      <p><strong>After Propensity Score Matching (Balanced Cohorts):</strong></p>
      <div class="metrics-row">
        <div class="metric-item">
          <div class="metric-label">SmartFeed_v2 7-Day Retention</div>
          <div class="metric-value">85.49%</div>
        </div>
        <div class="metric-item">
          <div class="metric-label">Control 7-Day Retention</div>
          <div class="metric-value">85.32%</div>
        </div>
        <div class="metric-item">
          <div class="metric-label">Treatment Effect</div>
          <div class="metric-value neutral">+0.17pp</div>
        </div>
      </div>
    </div>

    <h3>Segment-Level Analysis</h3>
    <table>
      <thead>
        <tr>
          <th>User Segment</th>
          <th>SmartFeed_v2 Retention</th>
          <th>Control Retention</th>
          <th>Difference</th>
          <th>Interpretation</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><strong>New Users</strong></td>
          <td>100% (n=3)</td>
          <td>100% (n=5)</td>
          <td>0pp</td>
          <td>Sample composition artifact; caution with small n</td>
        </tr>
        <tr>
          <td><strong>Returning Users</strong></td>
          <td>85.50%</td>
          <td>85.20%</td>
          <td>+0.30pp</td>
          <td>Neutral‚Äîno meaningful retention improvement</td>
        </tr>
        <tr>
          <td><strong>Power Users</strong></td>
          <td>84.98%</td>
          <td>-</td>
          <td>-</td>
          <td>Core segment shows no benefit; at risk</td>
        </tr>
      </tbody>
    </table>

    <div class="findings">
      <strong>‚úÖ Corrected Conclusion:</strong>
      <p>
        After matching users on propensity to be exposed to SmartFeed_v2, 
        <span class="highlight">the feature shows NO meaningful improvement in 7-day retention.</span>
        The treatment effect is negligible (+0.17 percentage points).
      </p>
    </div>

    <h3>Why the Results Differ from Naive Analysis</h3>
    <p>
      The naive analysis showed +144% engagement lift. PSM shows +0.17pp retention gain. 
      What happened?
    </p>

    <ul>
      <li>
        <strong>Selection Bias Revealed:</strong> Treated users (SmartFeed_v2) had inherently higher engagement propensity 
        (~0.45 avg score) than control users (~0.26 avg score). When we match on propensity, 
        we compare similar users‚Äîremoving the selection confound.
      </li>
      <li>
        <strong>Metric Gaming:</strong> SmartFeed_v2 increased short-term engagement (cards viewed) 
        through novelty or personalization appeal, but this didn't translate to long-term retention.
      </li>
      <li>
        <strong>Filter Bubble Effect:</strong> Personalized ranking may reduce content diversity, 
        leading users to get bored faster (no retention gain despite higher engagement).
      </li>
      <li>
        <strong>Power User Risk:</strong> Highly engaged users may view "personalized ranking" 
        as limiting their serendipitous discovery, leading to churn.
      </li>
    </ul>

    <div class="findings danger">
      <strong>üö® Critical Insight:</strong>
      <p>
        The +144% engagement lift was driven by <strong>user selection (who got the feature), 
        not the feature's causal impact.</strong> After controlling for selection via PSM, 
        the feature provides no retention benefit.
      </p>
    </div>
  </section>

  <!-- COMPARISON -->
  <section>
    <h2>Approach Comparison: Why PSM Wins</h2>

    <table>
      <thead>
        <tr>
          <th>Dimension</th>
          <th>Naive T-Test</th>
          <th>Propensity Score Matching</th>
          <th>Winner</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><strong>Confounding Control</strong></td>
          <td>None‚Äîcompares different user types</td>
          <td>Matches on propensity; removes selection bias</td>
          <td>PSM ‚úÖ</td>
        </tr>
        <tr>
          <td><strong>Metric Alignment</strong></td>
          <td>Engagement (cards viewed)</td>
          <td>Retention (north star metric)</td>
          <td>PSM ‚úÖ</td>
        </tr>
        <tr>
          <td><strong>Selection Bias</strong></td>
          <td>Not addressed; major risk</td>
          <td>Explicitly corrected via matching</td>
          <td>PSM ‚úÖ</td>
        </tr>
        <tr>
          <td><strong>Segment Analysis</strong></td>
          <td>Aggregated; hides differences</td>
          <td>Segment-level retention included</td>
          <td>PSM ‚úÖ</td>
        </tr>
        <tr>
          <td><strong>Causal Claim</strong></td>
          <td>Not valid from observational data</td>
          <td>Valid under conditional independence assumption</td>
          <td>PSM ‚úÖ</td>
        </tr>
        <tr>
          <td><strong>Business Impact</strong></td>
          <td>Misleading "green light" signal</td>
          <td>Accurate "red flag" signal</td>
          <td>PSM ‚úÖ</td>
        </tr>
      </tbody>
    </table>

    <h3>The Bottom Line</h3>
    <p>
      Naive analysis would have led to shipping a feature that provides no retention benefit. 
      PSM revealed the truth: the feature is driven by user selection, not product quality. 
      <strong>This is why careful causal inference matters in product decisions.</strong>
    </p>
  </section>

  <!-- RISKS & FINDINGS -->
  <section>
    <h2>Key Findings & Inference</h2>

    <h3>Finding 1: Selection Bias Dominates the Signal</h3>
    <div class="findings warning">
      <strong>The Data:</strong>
      <ul>
        <li>Treatment group avg engagement score: 0.45</li>
        <li>Control group avg engagement score: 0.26</li>
        <li>Difference: High-engagement users are 1.7x more likely to be in SmartFeed_v2</li>
      </ul>
      <p>
        <strong>Why It Matters:</strong> Comparing these groups directly conflates user characteristics 
        with treatment effects. PSM fixes this by matching on propensity.
      </p>
    </div>

    <h3>Finding 2: Engagement ‚â† Retention</h3>
    <div class="findings warning">
      <strong>The Data:</strong>
      <ul>
        <li>Naive analysis: +144% engagement (cards viewed)</li>
        <li>PSM analysis: +0.17pp retention (insignificant)</li>
      </ul>
      <p>
        <strong>Why It Matters:</strong> Users viewing more cards doesn't mean they're stickier. 
        Engagement can be driven by novelty or addiction without corresponding retention gains. 
        This is classic metric gaming.
      </p>
    </div>

    <h3>Finding 3: Power Users Are At Risk</h3>
    <div class="findings danger">
      <strong>The Data:</strong>
      <ul>
        <li>Power user 7-day retention (SmartFeed_v2): 84.98%</li>
        <li>No retention improvement despite high baseline engagement</li>
      </ul>
      <p>
        <strong>Why It Matters:</strong> Power users likely derive value from serendipitous discovery. 
        Personalized ranking restricts their content surface, reducing surprise value. Result: No retention gain.
      </p>
    </div>

    <h3>Finding 4: Novelty Effect is Likely</h3>
    <div class="findings">
      <strong>The Data:</strong>
      <ul>
        <li>Short-term engagement spike (cards viewed) at exposure</li>
        <li>No long-term retention benefit (7-day window)</li>
      </ul>
      <p>
        <strong>Why It Matters:</strong> Users interact with new features intensively out of curiosity. 
        Once novelty wears off, engagement and retention converge. SmartFeed_v2 shows this pattern.
      </p>
    </div>

    <h3>Finding 5: Potential Filter Bubble Risk</h3>
    <div class="findings warning">
      <strong>The Inference:</strong>
      <p>
        Personalized ranking by relevance scores optimizes for engagement on shown content 
        but may reduce content diversity (the platform's guardrail). 
        If users see fewer diverse perspectives, they may disengage long-term (no retention gain observed).
      </p>
    </div>
  </section>
<section>
  <h2>What Would Have Gone Wrong</h2>
  <ul>
    <li>Feature would ship based on false positive signal</li>
    <li>Power users may churn silently</li>
    <li>Retention stagnates despite rising engagement metrics</li>
    <li>Team learns the wrong lesson from metrics</li>
  </ul>
</section>

  <!-- RECOMMENDATIONS -->
  <section>
    <h2>Recommendation & Next Steps</h2>

    <div class="recommendation danger">
      <h3>üî¥ Do Not Ship SmartFeed_v2</h3>
      <p>
        The feature fails its success criteria: no meaningful retention benefit and poses risks to the power user segment.
      </p>

      <h4>Reasoning</h4>
      <ul>
        <li><strong>No North Star Improvement:</strong> 7-day retention shows +0.17pp (negligible) after controlling for selection bias</li>
        <li><strong>Metric Gaming Risk:</strong> +144% engagement without retention gain is a red flag for unsustainable optimization</li>
        <li><strong>Power User Churn Risk:</strong> Core segment sees no benefit; likely at risk of churn</li>
        <li><strong>Content Diversity Risk:</strong> Personalized ranking likely violates content diversity guardrail</li>
        <li><strong>New User Experience Risk:</strong> Complex personalization may confuse new users on signup</li>
      </ul>
    </div>

    <h3>Recommended Actions</h3>
    <div class="cards">
      <div class="card">
        <h4>1. Investigate Content Diversity</h4>
        <p>
          Measure content diversity in SmartFeed_v2 vs control. 
          If diversity score is down, it explains the retention plateau.
        </p>
      </div>
      <div class="card">
        <h4>2. Test Power Users Separately</h4>
        <p>
          Run a power user-only pilot to see if segment-specific variant improves retention. 
          May need to adjust personalization to preserve discovery.
        </p>
      </div>
      <div class="card">
        <h4>3. Redesign Ranking Strategy</h4>
        <p>
          Balance engagement + diversity + retention. Consider multi-objective optimization 
          rather than pure relevance ranking.
        </p>
      </div>
      <div class="card">
        <h4>4. Set Retention as Primary Target</h4>
        <p>
          Future features should optimize for 7-day (or longer) retention as primary metric, 
          not engagement spikes.
        </p>
      </div>
    </div>

    <h3>Lessons Learned</h3>
    <div class="findings">
      <strong>Key Takeaway:</strong>
      <p>
        <strong>Simple statistical tests (t-tests, basic A/B tests) are insufficient for causal inference 
        in observational data with self-selection.</strong> 
        Propensity score matching, difference-in-differences, or instrumental variables are essential when users 
        choose which treatment to receive based on their characteristics.
      </p>

      <p style="margin-top: 16px;">
        <strong>In this case:</strong>
      </p>
      <ul>
        <li>Engaged users self-selected into SmartFeed_v2 ‚Üí simple comparison was biased</li>
        <li>Propensity score matching corrected for this bias ‚Üí revealed the truth</li>
        <li>North star metric (retention) was secondary to vanity metric (engagement) ‚Üí false positive narrowly avoided</li>
      </ul>
    </div>
  </section>

  <!-- TECHNICAL APPENDIX -->
  <section>
    <h2>Technical Appendix: Propensity Score Matching Deep Dive</h2>

    <h3>Why PSM Works: The Intuition</h3>
    <p>
      In randomized experiments, treatment assignment is random. Users in treatment and control are 
      similar on all observable and unobservable characteristics‚Äîallowing valid causal comparison.
    </p>

    <p>
      In observational data (like this feature exposure), treatment is self-selected. Users in treatment 
      and control differ systematically (higher engagement propensity in treatment). This confounds the 
      treatment effect with user characteristics.
    </p>

    <p>
      <strong>PSM solution:</strong> Use propensity scores (predicted probability of treatment assignment) 
      to match treated and control users. After matching, treatment appears random (conditional on propensity), 
      enabling valid causal inference.
    </p>

    <h3>Conditional Independence Assumption</h3>
    <p>
      PSM assumes: <strong>After controlling for observed confounders (segment, engagement_score), 
      treatment assignment is independent of outcomes.</strong>
    </p>

    <p>
      In this case:
    </p>
    <ul>
      <li><strong>Observed confounders:</strong> user segment (new/returning/power), baseline engagement score</li>
      <li><strong>Assumption:</strong> After matching on these, SmartFeed_v2 assignment is independent of 7-day retention</li>
      <li><strong>Threat:</strong> Unobserved confounders (e.g., user intentions, time-of-day effects) could bias results</li>
    </ul>

    <p>
      In practice, PSM provides the best causal inference possible from observational data with measured confounders.
    </p>

    <h3>Limitations & Considerations</h3>
    <ul>
      <li>
        <strong>Small Sample Size:</strong> With only 13 users, statistical power is limited. 
        Segment-level retention rates are noisy (e.g., 100% for new users could be by chance).
      </li>
      <li>
        <strong>Unobserved Confounders:</strong> If unmeasured factors (e.g., user intentions) drive 
        both treatment assignment and retention, PSM won't capture them.
      </li>
      <li>
        <strong>Common Support:</strong> Matching requires overlap in propensity scores between treatment and control. 
        With small samples, some users may not have good matches.
      </li>
      <li>
        <strong>Short Follow-Up:</strong> 7-day retention is early. Longer-term retention (30-day) would be stronger evidence.
      </li>
    </ul>

    <h3>When to Use PSM vs. Other Methods</h3>
    <table>
      <thead>
        <tr>
          <th>Situation</th>
          <th>Best Method</th>
          <th>Why</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Randomized experiment (you control assignment)</td>
          <td>Simple A/B test / t-test</td>
          <td>No confounding; comparison is valid</td>
        </tr>
        <tr>
          <td>Observational data, users self-select based on observed confounders</td>
          <td>Propensity Score Matching (PSM)</td>
          <td>Removes selection bias on observed characteristics</td>
        </tr>
        <tr>
          <td>Observational data, multiple cohorts over time</td>
          <td>Difference-in-Differences (DiD)</td>
          <td>Controls for time trends and time-invariant confounders</td>
        </tr>
        <tr>
          <td>Observational data, strong unobserved confounding feared</td>
          <td>Instrumental Variables (IV)</td>
          <td>Uses external instrument to isolate causal effect</td>
        </tr>
      </tbody>
    </table>
  </section>

  <!-- CONCLUSION -->
  <section>
    <h2>Conclusion</h2>

    <p>
      <strong>This case study demonstrates the critical importance of rigorous statistical methodology in product decisions.</strong>
    </p>

    <p>
      A naive analysis would have shown a 144% engagement lift and recommended shipping SmartFeed_v2. 
      But deeper statistical analysis revealed the truth: the feature provides no retention benefit and 
      poses risks to power users.
    </p>

    <p>
      The root cause was <strong>selection bias</strong>‚Äîmore engaged users self-selected into the feature, 
      confounding user characteristics with the treatment effect. Propensity score matching corrected for this, 
      revealing the feature's true causal impact (negligible).
    </p>

    <p>
      <strong>Key Lessons:</strong>
    </p>
    <ul>
      <li>Always use the north star metric (retention), not vanity metrics (engagement)</li>
      <li>Be skeptical of impressive metric lifts in observational data without proper causal controls</li>
      <li>When users self-select into features, use causal inference techniques (PSM, DiD, IV)</li>
      <li>Segment-level analysis reveals heterogeneous treatment effects and risks</li>
      <li>Metric gaming (engagement ‚Üë without retention ‚Üë) is a common signal of flawed features</li>
    </ul>

    <div class="findings">
      <strong>The Decision:</strong>
      <p>
        <strong>üî¥ Rollback SmartFeed_v2.</strong> Redesign personalization strategy to optimize for 
        retention while maintaining content diversity and power user experience.
      </p>
    </div>
  </section>

  <footer>
    ¬© 2025 Abhitay Shinde | Feature Impact & Experimentation Case Study
  </footer>

</body>
</html>
