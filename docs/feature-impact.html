<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Feature Impact Analysis | Causal Inference Case Study</title>

  <link rel="stylesheet" href="styles/main.css">
</head>

<body>

  <!-- NAVBAR -->
  <nav>
    <div class="left"><a href="index.html">Abhitay Shinde</a></div>
    <div class="right">
      <a href="index.html#projects">Highlights</a>
      <a href="index.html#experience">Background</a>
      <a href="index.html#contact">Contact</a>
      <a class="btn" href="Shinde Resume.pdf" target="_blank">Download Resume</a>
    </div>
  </nav>

  <!-- HERO -->
  <section class="hero">
    <h1>When Engagement Lifts Mislead: Did the New Feed Improve Retention?</h1>
    <p class="subtitle">
      Why the initial engagement signal mattered — and how causal correction changed the decision.
    </p>
    <div class="tech-tags">
      <span class="tag">Product Analytics</span>
      <span class="tag">Causal Inference</span>
      <span class="tag">Propensity Score Matching</span>
      <span class="tag">Logistic Regression</span>
    </div>
  </section>

  <!-- TLDR -->
   <section>
  <section class="tldr">
    <h2>TL;DR</h2>
    <p>
      A new feed iteration appeared to drive a <strong>~110–140% lift in engagement</strong>.
      After correcting for selection bias using <strong>propensity score matching</strong>,
      I found <strong>no meaningful improvement in 7-day retention</strong>.
      The lift was driven by <em>who</em> received the feature — not the feature itself.
    </p>
    <p><strong>Final decision: do not ship the feature.</strong></p>
  </section></section>
  <!-- CONTEXT -->
  <section>
    <h2>The Decision That Triggered This Analysis</h2>
    <p>
      This case study evaluates a feed-ranking change in a generic consumer social app.
      The feed is a core surface for engagement and long-term retention, making even small
      ranking changes high-risk.
    </p>
    <p>
      The feature introduced more aggressive personalization based on predicted relevance.
      Exposure was <strong>not randomized</strong>, making this an observational analysis
      rather than a clean A/B test.
    </p>
  </section>

  <!-- WHO WE WERE COMPARING -->
  <section>
    <h2>Who We Were Comparing</h2>
    <p class="section-note">
      Users clustered naturally into three behavioral segments.
    </p>

    <div class="split">
      <div>
        <div class="cards">
          <div class="card compact">
            <h4>Low-engagement users</h4>
            <p>Infrequent usage, low baseline interaction</p>
          </div>
          <div class="card compact">
            <h4>Normal users</h4>
            <p>Moderate engagement, majority of the base</p>
          </div>
          <div class="card compact">
            <h4>Power users</h4>
            <p>Highly engaged, critical to platform health</p>
          </div>
        </div>

        <p style="margin-top:16px;">
          These segments matter because engagement propensity differs sharply across them.
        </p>
      </div>

      <div class="card figure-only">
        <div class="fig">
          <img src="../feature_impact/figures/1a.png" alt="User Segment Distribution">
        </div>
      </div>
    </div>
  </section>

  <!-- METRICS -->
<section>
  <h2>Why Standard Metrics Were Misleading</h2>
  <p class="lead">
    Engagement spikes are diagnostic, not decisive. Success criteria were defined upfront.
  </p>

  <div class="two-col metrics-contrast">

    <!-- NORTH STAR -->
    <div class="card metric-primary">
      <span class="metric-label">North Star Metric</span>
      <h3>7-day retention</h3>
      <p class="muted">
        Best proxy for habit formation and long-term value.
      </p>
    </div>

    <!-- DIAGNOSTICS -->
    <div class="card metric-secondary">
      <span class="metric-label">Diagnostic Metrics</span>
      <ul class="clean-list">
        <li>Cards viewed per session</li>
        <li>Bounce rate</li>
      </ul>
      <p class="muted">
        Useful for understanding behavior, not for making ship decisions.
      </p>
    </div>

  </div>

  <div class="findings">
    <strong>Decision rule</strong>
    <p>
      The decision hinged on whether users returned — not whether they interacted more in a single session.
    </p>
  </div>
</section>



  <!-- DATA -->
<section>
  <h2>The Data Behind the Decision</h2>

  <p class="lead">
    The analysis relied on two complementary datasets that together made it possible
    to separate <strong>user quality</strong> from <strong>feature impact</strong>.
  </p>

  <!-- DATA OVERVIEW -->
  <div class="two-col data-sources">

    <!-- USERS TABLE -->
    <div class="card">
      <span class="metric-label">Users Table</span>
      <p class="muted">
        One row per user. Used to model selection bias and long-term outcomes.
      </p>
      <ul class="clean-list">
        <li>User segment (low / normal / power)</li>
        <li>Baseline engagement score (pre-exposure)</li>
        <li>Feature exposure flag</li>
        <li>7-day retention outcome</li>
      </ul>
    </div>

    <!-- EVENTS TABLE -->
    <div class="card">
      <span class="metric-label">Events Table</span>
      <p class="muted">
        Session-level behavioral data. Used to measure short-term engagement.
      </p>
      <ul class="clean-list">
        <li>Session start / end events</li>
        <li>Card view events per session</li>
        <li>Feature flag at time of interaction</li>
      </ul>
    </div>

  </div>

  <!-- WHY THIS DATA WORKS -->
  <div class="cards">
    <div class="card">
      <h4>Unit of analysis</h4>
      <p>
        Users for retention analysis; sessions for engagement diagnostics.
      </p>
    </div>

    <div class="card">
      <h4>Time window</h4>
      <p>
        Baseline behavior measured pre-exposure, outcomes tracked over 7 days post-exposure.
      </p>
    </div>

    <div class="card">
      <h4>Why this matters</h4>
      <p>
        Separating baseline behavior from outcomes makes causal adjustment possible.
      </p>
    </div>
  </div>

  <!-- KEY SIGNAL -->
  <div class="findings warning">
    <strong>Key data signal</strong>
    <p>
      Users exposed to the new feature had systematically higher baseline engagement,
      indicating strong selection bias in feature exposure.
    </p>
  </div>

  <p class="lead">
    This meant naive engagement comparisons primarily reflected
    <strong>who received the feature</strong>,
    not <strong>what the feature caused</strong>.
  </p>
</section>



 <section>
  <h2>The Naive Comparison (Why It Deceived Us)</h2>
  <p class="lead">A dashboard view suggested a dramatic engagement win.</p>

  <div class="two-col">

    <!-- LEFT COLUMN -->
    <div class="stack">

      <!-- Metrics card -->
      <div class="card">
        <h4>Average cards viewed per session</h4>

        <div class="metric-row">
          <span>Control users</span>
          <strong>~2.3</strong>
        </div>

        <div class="metric-row">
          <span>New feature users</span>
          <strong>~4.9</strong>
        </div>

        <hr />

        <div class="metric-row highlight-row">
          <span>Observed lift</span>
          <strong class="negative">~110–140%</strong>
        </div>
      </div>

      <!-- Interpretation card -->
      <div class="card subtle">
        <strong>Why this looked convincing</strong>
        <p>
          At face value, this appears to be a major engagement win.
          A standard dashboard would strongly suggest shipping.
        </p>
      </div>

    </div>

    <!-- RIGHT COLUMN -->
    <div class="card figure-only">
      <div class="fig">
        <img src="../feature_impact/figures/1b.png" alt="Naive Engagement Comparison">
      </div>
      <p class="caption">
        Takeaway: the lift conflates user quality with feature impact.
      </p>
    </div>

  </div>
</section>


  <!-- FAILURE -->
<section>
  <h2>Why the Naive Comparison Failed</h2>

  <div class="findings danger">
    <strong>Core issue</strong>
    <p>
      Exposure to the new feed was non-random and strongly correlated with baseline engagement.
    </p>
  </div>

  <p class="lead">
    The comparison measured <strong>high-engagement users vs low-engagement users</strong>,
    not the causal effect of the feature.
  </p>

  <p class="muted">
  Analogy: comparing basketball players to accountants and concluding basketball makes people taller.
</p>

</section>


  <!-- METHOD -->
  <section>
    <h2>How I Isolated the Causal Effect</h2>
    <p>
      I applied propensity score matching to approximate a randomized experiment.
    </p>

    <div class="cards three-col">
      <div class="card compact">
        <h4>Non-random treatment</h4>
        <p>Assignment depended on behavior.</p>
      </div>
      <div class="card compact">
        <h4>Observable confounders</h4>
        <p>Segment and baseline engagement.</p>
      </div>
      <div class="card compact">
        <h4>RCT approximation</h4>
        <p>Matched treated and control users.</p>
      </div>
    </div>
  </section>

  <!-- RESULTS -->
  <section>
  <h2>What Changed After Correcting for Bias</h2>
  <p class="lead">
    Matching shifted the comparison onto like-for-like users.
  </p>

  <div class="two-col">

    <!-- LEFT COLUMN -->
    <div class="stack">

      <!-- Metrics card -->
      <div class="card">
        <h4>7-day retention after adjustment</h4>

        <div class="metric-row">
          <span>New feature retention</span>
          <strong>~71%</strong>
        </div>

        <div class="metric-row">
          <span>Control retention</span>
          <strong>~73%</strong>
        </div>

        <hr />

        <div class="metric-row highlight-row">
          <span>Treatment effect</span>
          <strong class="neutral">≈ 0</strong>
        </div>
      </div>

      <!-- Interpretation card -->
      <div class="card subtle positive">
        <strong>What this tells us</strong>
        <p>
          Once comparable users are evaluated, the apparent engagement win
          disappears. The feature does not improve retention.
        </p>
      </div>

    </div>

    <!-- RIGHT COLUMN -->
    <div class="card figure-only">
      <div class="fig">
        <img src="../feature_impact/figures/1d.png" alt="7-Day Retention After Propensity Score Matching">
      </div>
      <p class="caption">
        Retention remains unchanged after causal adjustment.
      </p>
    </div>

  </div>
</section>

  <!-- RECOMMENDATION -->
  <section>
    <h2>Recommendation</h2>
    <div class="findings danger">
      <strong>Do not ship the feature.</strong>
      The feature failed to improve the north-star metric and introduces risk.
    </div>
  </section>

  <footer>
    © 2025 Abhitay Shinde | Feature Impact & Experimentation Case Study
  </footer>

</body>
</html>
